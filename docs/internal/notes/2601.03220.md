# Entropy to Epiplexity in Bounded Intelligence

## Abstract

Can we learn more from data than existed in the generating process itself? Can new and useful information be constructed from merely applying deterministic transformations to existing data? Can the learnable content in data be evaluated without considering a downstream task? On these questions, Shannon information and Kolmogorov complexity come up nearly empty-handed, in part because they assume observers with unlimited computational capacity and fail to target the useful information content. In this work, we identify and exemplify three seeming paradoxes in information theory: (1) information cannot be increased by deterministic transformations; (2) information is independent of the order of data; (3) likelihood modeling is merely distribution matching. To shed light on the tension between these results and modern practice, and to quantify the value of data, we introduce epiplexity, a formalization of information capturing what computationally bounded observers can learn from data. Epiplexity captures the structural content in data while excluding time-bounded entropy, the random unpredictable content exemplified by pseudorandom number generators and chaotic dynamical systems. With these concepts, we demonstrate how information can be created with computation, how it depends on the ordering of the data, and how likelihood modeling can produce more complex programs than present in the data generating process itself. We also present practical procedures to estimate epiplexity which we show capture differences across data sources, track with downstream performance, and highlight dataset interventions that improve out-of-distribution generalization. In contrast to principles of model selection, epiplexity provides a theoretical foundation for data selection, guiding how to select, generate, or transform data for learning systems.

## From Entropy to Epiplexity: Rethinking Information for Computationally Bounded Intelligence

## Introduction and Motivation

This work introduces **epiplexity** as a formalism for the structural information content available to a computationally bounded observer, fundamentally refining the role of information theory in machine learning. The classical frameworks of Shannon entropy and Kolmogorov complexity operate under the assumption of unbounded computation, classifying all deterministic transformations and orderings as information-preserving (modulo constant overheads) and treating inference and sampling as symmetric tasks. However, as the authors demonstrate, these classical notions do not adequately explain phenomena observed in modern machine learning, where compute, data ordering, and information transfer asymmetries critically impact outcomes.

Three core paradoxes in Shannon and algorithmic information theory are identified:

1. **Information non-increase under deterministic transformations:** Deterministic post-processing cannot increase information.
2. **Order-invariance:** Information content does not depend on sampling or factorization order.
3. **Likelihood modeling as mere distribution matching:** Likelihood maximization simply fits the data law, not uncovering or constructing latent structures not present in the generative process.

Experimental and theoretical developments in this work show that these statements—although true under classical definitions—are violated when incorporating computationally bounded observers, with significant practical and theoretical implications.

(Figure 1)

*Figure 1: Illustration of random vs structural information for computationally-bounded observers, as decomposed by time-bounded entropy and epiplexity.*

## Formalization of Epiplexity

The central technical contribution is the definition of epiplexity, grounded in minimum description length (MDL) and drawing on cryptographic indistinguishability (pseudoentropy):

Given a random variable $X$ and time bound $T$, the epiplexity $S_T(X)$ and time-bounded entropy $H_T(X)$ are defined via the MDL-optimal probabilistic model $P^*$ subject to computational constraints. $S_T(X)$ is the number of bits for describing $P^*$, and $H_T(X)$ is the expected codelength of $X$ under $P^*$. This decomposition strictly depends on computational bounds; with unlimited compute, only truly random objects retain non-trivial information content, and all structure can be collapsed into negligible information.

Classical cryptography is leveraged to distinguish random (incompressible) and pseudorandom information. For polynomial-bounded observers, constructions such as CSPRNGs exhibit maximal time-bounded entropy while having only constant epiplexity—the efficient program running the generator. This provides strong theoretical support for the decomposition.

## Measurement Approaches: Prequential and Requential Coding

Two empirical proxies for estimating epiplexity and time-bounded entropy are developed and evaluated:

- **Prequential Coding:** Measures the area under the training loss curve above the final loss, providing a practical, albeit heuristic, upper bound.
- **Requential Coding:** Utilizes student-teacher training curves and cumulative KL divergence between checkpoints, yielding a rigorous coding of the model achieving the loss.

Prequential coding is computationally efficient and aligns well with requential coding in terms of rank correlation in practical scenarios. The pipeline for estimation and compute-optimal hyperparameter selection is well-detailed.

(Figure 2)

*Figure 2: Approaches for efficiently estimating epiplexity and illustrating differences between prequential and requential coding.*

## Analysis of Information-Theoretic Paradoxes

### 1. Deterministic Information Creation

The pseudoentropy perspective demonstrates that computational processes (such as CSPRNGs, cellular automata, or even systems like AlphaZero) produce information that is irreducible without the associated computation. This is empirically validated via transformers trained on data from cellular automata, exhibiting distinct patterns of time-bounded entropy and epiplexity for simple, complex, and chaotic evolution rules. The framework shows that, contrary to classical DPI, deterministic computation can increase time-bounded entropy and, fundamentally, the *learnable* portion of information.

(Figure 3)

*Figure 3: Information creation in cellular automata—randomness and structure detailed by rule type.*

### 2. Factorization and Order Asymmetry

Experiments in modeling one-way functions (e.g., cellular automata with noninvertible dynamics, and move/board orders in chess corpora) show that ordering has a concrete effect on both time-bounded entropy and epiplexity. The modeling capacity and the learnable structure are sensitive to the conditional factorization chosen, violating the symmetry of information and Bayesian reversibility for bounded observers.

(Figure 4)

*Figure 4: Differences in losses for one-way function modeling and chess data factorization—clear asymmetries with respect to modeling direction.*

### 3. Beyond Distribution Matching: Likelihood Modeling, Induction, and Emergence

Inductive reasoning in machine learning is dissected in latent-variable construction tasks where information required for likelihood computation exceeds that for generation. For instance, predicting masked outputs in cellular automata after partial observation or sampling Markov chains with missing transitions, models must develop complex internal circuits (not present in the generative process) to deduce the missing information, increasing epiplexity.

Emergence is also treated as a concrete instance where bounded observers must discover and internalize higher-order structures to achieve accurate prediction, as in iterated dynamics of cellular automata. In such regimes, brute-force iteration of transition rules is infeasible; thus, models capture and compress emergent macro-structures, leading to elevated epiplexity.

(Figure 5)

*Figure 5: Induction experiments showing that as models are forced to consider latent/hard structure, epiplexity increases.*

## Practical Implications: OOD Generalization and Data Selection

A salient consequence is that epiplexity aligns with a model's potential for OOD transfer. Data and orderings that induce greater epiplexity facilitate broader generalization, as supported by downstream evaluation of models trained on different chess data orderings. Higher epiplexity in pre-training is predictive of superior performance in OOD-centric tasks (e.g., chess evaluation), even when in-distribution loss remains unchanged.

Additionally, language data is shown to produce higher epiplexity than other modalities (e.g., images, videos), rationalizing the empirical success of text-pretraining for transfer. At scale, the paper estimates epiplexity for various modalities, validating that differences explain the variance in generalization efficacy.

(Figure 6)

*Figure 6: Estimated epiplexity and time-bounded entropy in language, chess, and image datasets, aligning with observed generalization capabilities.*

(Figure 7)

*Figure 7: Pareto frontier estimation for training compute versus epiplexity, illustrating the need for careful hyperparameter selection and data curation.*

Innovations such as Adaptive Data Optimization (ADO) implicitly increase epiplexity, and the paper provides evidence that such interventions yield gains in OOD performance proportional to the induced structural complexity.

## Theoretical and Empirical Limitations

While the analytical framework formalizes task-agnostic data value as structural information for a computationally bounded observer, it does not guarantee transfer to any specific downstream distribution. Epiplexity, by construction, is observer- and model-class-dependent, and estimation can be computationally intensive. Moreover, while empirical evidence for emergence is strong, fully formal lower bounds for specific tasks remain open.

## Discussion and Future Directions

Epiplexity repositions information as a resource relative to computationally bounded observers. This reframing enables explanations for the empirical discrepancy between data curation, synthetic data utility, transfer learning, and the role of emergent phenomena in model performance. The proposed framework suggests a new theoretical axis—complementary to classical sample complexity—centering the *structural information load* required for generalization under compute constraints.

Potential future developments include:

- Compute-aware analogues for sufficiency and bottlenecks.
- Finer-grained emergence characterizations linking computational constraints to epistemic phase transitions.
- Systematic strategies for constructing high-epiplexity synthetic data.

## Conclusion

This work provides a rigorous formalization and empirical methodology for quantifying the structural information learnable from data by computationally bounded observers. By decomposing information into time-bounded entropy and epiplexity, it resolves several theoretical paradoxes inherent in classical information theory when applied to modern machine learning. The implications span better pre-training curricula, principled synthetic data construction, and deeper understanding of emergence and induction in AI systems. The framework positions epiplexity as an essential and quantifiable property of data, model, and computation triads in contemporary AI research.

Source: https://www.emergentmind.com/papers/2601.03220